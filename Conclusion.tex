$ _{} $ $ _{} $ $ _{} $ $ _{} $ $ _{} $Ce travail est un rapport accompagnant le système que nous avons mis au point en guise de mémoire. Le système en question a pour rôle de générer automatiquement les résumés des documents qui lui sont fournis.

Au début de ce travail, nous avons fait quelques hypothèses. Dans la première hypothèse, nous affirmons qu'un traitement purement linguistique ne suffirait pas pour pouvoir produire des résumés performants en utilisant un système informatique. A l'issu de nos recherches, et des expérimentations, nous avons pu vérifier cette hypothèse car, nous avons démontré que le traitement purement linguistique du langage naturel est très gour\-mand en ressources et très fastidieux. En plus, le langage naturel n'est pas aussi précis que peut l'être un langage formel quelconque.\\
$ _{} $ $ _{} $ $ _{} $ $ _{} $ $ _{} $A la seconde hypothèse, nous avons avancé qu'on ne peut se passer de l'intelligence artificielle si on veut obtenir des systèmes de résumé de performance presqu'humaine. Cela se vérifie à travers la littérature que nous avons présentée, ainsi que la complexité inhérente au traitement du langage naturel tel que nous l'avons montré dans ce travail. L'usage de l'intelligence artificielle permet de réaliser des tâches à grande complexité à un coût faible par rapport à une approche algorithmique détaillée.\\
$ _{} $ $ _{} $ $ _{} $ $ _{} $ $ _{} $La troisième et dernière hypothèse que nous avons émise était qu'une architecture du type \textit{transformer}, à laquelle on joint des traitements lin\-guis\-ti\-ques sommaires comme la segmentation du texte par exemple, permettrait d'obtenir des bons systèmes de résumé automatique. Cette hypothèse doit être précisée au vu des études que nous avons menées à travers ce travail. En effet, les recherches que nous avons menées nous ont permis de nous rendre compte que, pour des systèmes destinés à un usage en production, il est mieux de se limiter aux algorithmes classiques pour la synthèse extractive au lieu d'utiliser des systèmes d'intelligence artificielle très lourds. Cela permet de gagner en performance et en expérience utilisateur. Néanmoins, grâce aux modèles de \textit{deep learning} basés sur les mécanismes d'attention (les \textit{transformers} en l'occurence), on peut se passer de la nécessité d'implémenter explicitement les règles lin\-guis\-ti\-ques car le pré-entraînement des \textit{transformers} capture déjà les règles de la langue considérée. Ainsi, les règles linguistiques à implémenter peuvent être réduites au strict minimum. C'est pour cela que, bien que les modèles soient assez lourds en général, ils sont incontournables si on veut obtenir des résumés de qualité presqu'humaine (abstractifs), et corriger les erreurs de coupure de sens inhérentes à la synthèse par sélection brute des phrases saillantes (extraction). Ainsi donc, nous avons montré qu'on ne peut se passer des modèles d'intelligence artificielle si on veut obtenir des systèmes de résumé automatique performants, surtout que tous les aspects du langage naturel ne peuvent pas être formalisés.\\
$ _{} $ $ _{} $ $ _{} $ $ _{} $ $ _{} $L'objectif de ce travail était, comme énoncé dans l'introduction générale, de concevoir et réaliser un système, en l'occurrence une application web, devant permettre la génération automatique des résumés des textes. Pour atteindre ledit objectif, il nous a fallu réaliser $ 6 $ objectifs spécifiques.\\
$ _{} $ $ _{} $ $ _{} $ $ _{} $ $ _{} $Le premier objectif, concernant l'évaluation des failles et limites des techniques de synthèse existantes a été mené à terme. Il a débouché sur le constat selon lequel, les systèmes de résumé extractif par usage d'algorithmes classiques (sans traitement intelligent) sont couramment utilisés à cause de la facilité d'implémentation mais sont sujets aux erreurs de coupure de sens, étant donné que les phrases saillantes sont sélectionnées en les enlevant de leur contexte initial. Ce problème peut être réduit en essayant d'exclure au maximum les éléments hors contexte. Malheureusement, cette dernière manipulation a ses limites car elle ne peut pas supprimer complètement les coupures de sens inhérents au processus même de synthèse extractif. C'est pourquoi, les modèles abstractifs se présentent comme modèle de choix pour résoudre ce problème. Nous avons également montré que, comme le module \textit{gensim} de python a moins de chance de capturer un large éventail des points saillants dans les textes, l'utilisation d'un algorithme qui combinerait plusieurs approches de synthèse extractive a plus de chances de capturer une multitudes d'aspects. C'est pourquoi nous avons implémenté un algorithme de synthèse extractive fonctionnant par combinaison de divers algorithmes qui ont chacun sa spécificité. Cela a contribué à une nette amélioration qualitative sur les résultats, ce qui fait partie du second objectif de ce travail qui consistait à corriger le cas échéant, les failles observées.\\
$ _{} $ $ _{} $ $ _{} $ $ _{} $ $ _{} $Et, s'agissant justement de ce second objectif, qui consiste à apporter les améliorations nécessaires, nous devons préciser que, pour la synthèse abstractive, que nous avons réalisé grâce aux modèles du type \textit{transformer}, nous avons implémenté un pipeline qui permet de réaliser la synthèse des très longs documents (allant au-delà de $ 100 $ pages) alors qu'au préalable, les \textit{transformers} utilisés se limitent à environ $ 1 $ page.\\
$ _{} $ $ _{} $ $ _{} $ $ _{} $ $ _{} $Le troisième objectif consistant à établir une architecture logique fonctionnelle et optimale pour obtenir des synthèse de qualité a été atteint, d'abord à travers l'optimisation que nous avons faite durant l'usage du modèle \textit{BART} pour la synthèse abstractive (et cela contribue également au second objectif du travail), puis à travers l'architecture globale que nous avons utilisé pour notre système. Il s'agit en fait d'une architecture \textit{3-tiers} classique mais qui est in\-trin\-sè\-que\-ment évolutive étant donné la séparation nette faite entre \textit{API} et \textit{client web}, auquel on joint un stockage continu des résumés obtenus.\\
$ _{} $ $ _{} $ $ _{} $ $ _{} $ $ _{} $On conclut, de ce qui  vient d'être mentionné, que le quatrième objectif, consistant à élaborer une \textit{API} devant servir de moteur de résumé a également été atteint, ainsi que le très important cinquième objectif consistant à mettre au point une base des données devant nous permettre de constituer à la longue un \textit{dataset} riche, susceptible d'être utilisé pour élaborer des modèles de résumé de plus en plus performants.\\
$ _{} $ $ _{} $ $ _{} $ $ _{} $ $ _{} $Le sixième et dernier objectif de ce travail, consistant à mettre au point une interface web devant consommer les services de l'\textit{API} a également été atteint au vu du système que nous avons réalisé.\\
Les $ 6 $ objectifs spécifiques ayant été atteints, nous estimons avoir mis au point un écosystème complet de synthèse automatique des documents.\\
$ _{} $ $ _{} $ $ _{} $ $ _{} $ $ _{} $Nous devons néanmoins préciser que ce système est mieux adapté à la synthèse générique des documents de type narratif, argumentatif et littéraire en général (romans, nouvelles, discours, articles de presse, documents à faible densité mathématique et figures,...).\\
Pour la synthèse extractive, nous avons utilisé deux approches comme nous l'avons déjà mentionné (le module \textit{gensim} de python et \textit{merging} que nous avons mis au point en combinant plusieurs méthodes de synthèse extractive).\\
$ _{} $ $ _{} $ $ _{} $ $ _{} $ $ _{} $Pour la synthèse abstractive, celle qui se rapproche le plus de la synthèse humaine, nous avons utilisé des modèles du type \textit{BART}, pour éviter de pousser trop loin l'abstraction comme le ferait \textit{PEGASUS}, et en même temps éviter d'utiliser un modèle inutilement trop général pour notre contexte, comme c'est le cas du modèle \textit{T5}.\\
$ _{} $ $ _{} $ $ _{} $ $ _{} $ $ _{} $Nous avons également mis en place un \textit{dataset} de résumé automatique que nous avons nommé \textit{for-ULPGL-Dissertation} qui accueillera au fil du temps les données de synthèse recueillies à travers la base des données de notre système (que nous avons nommé \textit{Mon Résumeur}) et deviendra de plus en plus riche et performant. Ce \textit{dataset}, dans sa version actuelle (pas encore très riche) a été utilisé pour mettre au point le modèle \textit{BATkrame-abstract} basé sur le modèle \textit{mBARThez}, lui-même basé sur \textit{mBART} que nous continuerons également à améliorer en même temps que le \textit{dataset}. Finalement, nous avons utilisé le paquet \textit{ROUGE} comme métrique d'évaluation de notre système.\\

Ce domaine est très intéressant et surtout très vaste, c'est pourquoi l'ouverture de ce sujet est très large. Ainsi, aux futurs étudiants, qui comptent améliorer ce travail ou tout simplement œuvrer dans le même domaine, nous pouvons suggérer :
\begin{itemize}
\item[•] d'implémenter, selon leurs besoins, d'autres clients web pouvant consommer les services de cette \textit{API} mise au point;
\item[•] d'étudier minutieusement les limites de la métrique \textit{ROUGE} ainsi que d'autres métriques d'évaluation des systèmes génératifs pour mettre au point, le cas échéant, des nouvelles métriques, aussi moins gourmandes en ressources que \textit{ROUGE} mais plus performantes en matière d'évaluation de qualité;
\item[•] d'étendre notre système de résumé automatique au \textit{résumé guidé} et multi-documents;
\item[•] d'étendre le processus au résumé des vidéos ou même d'audios tout simplement;
\item[•] d'étudier la possibilité de mise au point des systèmes devant permettre la synthèse des documents à forte teneur mathématique;
\item[•] d'étudier la possibilité de réaliser un résumeur universel, c'est-à-dire qui serait adapté à la majorité de types de documents connus (documents spécialisés, littéraires,...);
\item[•] d'optimiser le temps de calcul des modèles de synthèse automatique;
\item[•] etc
\end{itemize}